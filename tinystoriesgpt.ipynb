{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1>Import Dependencies</h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:11.110450Z","iopub.status.busy":"2024-03-11T00:14:11.109414Z","iopub.status.idle":"2024-03-11T00:14:15.295465Z","shell.execute_reply":"2024-03-11T00:14:15.294399Z","shell.execute_reply.started":"2024-03-11T00:14:11.110389Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import regex as re # for the regex pattern that the tokenizer will use\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Load the data into each component(validation and the train)</h1>"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:15.298661Z","iopub.status.busy":"2024-03-11T00:14:15.297626Z","iopub.status.idle":"2024-03-11T00:14:15.703182Z","shell.execute_reply":"2024-03-11T00:14:15.702272Z","shell.execute_reply.started":"2024-03-11T00:14:15.298617Z"},"trusted":true},"outputs":[],"source":["val_text = pd.read_csv(\"/kaggle/input/tinystories-narrative-classification/validation.csv\").text # strip the text for each story"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:15.704886Z","iopub.status.busy":"2024-03-11T00:14:15.704521Z","iopub.status.idle":"2024-03-11T00:14:56.803359Z","shell.execute_reply":"2024-03-11T00:14:56.802438Z","shell.execute_reply.started":"2024-03-11T00:14:15.704854Z"},"trusted":true},"outputs":[],"source":["train_text = pd.read_csv(\"/kaggle/input/tinystories-narrative-classification/train.csv\").text"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.806857Z","iopub.status.busy":"2024-03-11T00:14:56.806434Z","iopub.status.idle":"2024-03-11T00:14:56.814617Z","shell.execute_reply":"2024-03-11T00:14:56.813619Z","shell.execute_reply.started":"2024-03-11T00:14:56.806811Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(21990, 2119719)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# checking how many stories there are in each split\n","# i tried to use them all but the datasets took an extraordinary amount of time to compile for training\n","# you will later see that I only use 4000 stories from the training split and 1000 stories from the testing split\n","len(val_text), len(train_text)"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Class Definitions</h1>\n","<ul>\n","    <li>\n","        Tokenizer\n","    </li>\n","    <li>\n","        GPT\n","        <ul>\n","            <li>\n","                Feed Forward\n","            </li>\n","            <li>\n","                Multihead Attention\n","            </li>\n","            <li>\n","                Decoder Block\n","            </li>\n","            <li>\n","                Full Definition\n","            </li>\n","        </ul>\n","    </li>\n","</ul>"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T01:04:29.810346Z","iopub.status.busy":"2024-03-11T01:04:29.809922Z","iopub.status.idle":"2024-03-11T01:04:29.838050Z","shell.execute_reply":"2024-03-11T01:04:29.836999Z","shell.execute_reply.started":"2024-03-11T01:04:29.810308Z"},"trusted":true},"outputs":[],"source":["class Tokenizer():\n","    \"\"\"\n","    Trains a tokenizer for a small GPT specifically for the Tiny Stories dataset.\n","    Uses the GPT4 regex pattern to break the text into chunks to prevent the merging of inconvenient tokens.\n","    Adapted from https://github.com/karpathy/minbpe/blob/master/minbpe/regex.py.\n","    Made to handle special tokens in a more simple manner.\n","    \"\"\"\n","    \n","    \n","    def __init__(self, pattern):\n","        \n","        # initialize vocab size to include the standard 256 vocab set of characters\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n","        self.vocab_size = 256\n","        \n","        # merges table for use when encoding and decoding after training the tokenizer\n","        self.merges = {}\n","        \n","        # stores the pattern that we will use (this is mostly useless because it only gets used to compile pattern)\n","        self.pattern = pattern\n","        \n","        # compiles the regex pattern that will be used to break the given text into chunks based on some rules that are used for GPT4\n","        self.compiled_pattern = re.compile(pattern)\n","        \n","        # stores the special tokens that we register\n","        self.special_tokens = {}\n","    \n","    \n","    def register_special_tokens(self, special_tokens):\n","        \n","        # for each token in the given list of special_tokens to register\n","        for token in special_tokens:\n","            \n","            # if this is not a token we have already registered\n","            if token not in self.special_tokens:\n","                \n","                # add an entry into our special_tokens dictionary and map it to the corresponding index that it will be represented by\n","                self.special_tokens[token] = self.vocab_size\n","                \n","                # add it to our vocabulary under its index representation and map it to the raw bytes of its encoding\n","                self.vocab[self.vocab_size] = bytes(token.encode(\"utf-8\"))\n","                \n","                # reflect in our vocab_size that a new vocab word was added\n","                self.vocab_size += 1\n","    \n","    \n","    def get_stats(self, ids, counts = None):\n","        \"\"\"\n","        Given a list of integers (ids in our context), this function computes the frequencies of every pair of ids that occur\n","        If we are given a list of counts to use as our base for frequencies we use it, if not we start from scratch\n","        \"\"\"\n","        \n","        # here is the logic to implement the optional counts passed to the function\n","        counts = {} if counts is None else counts\n","        \n","        # for each pair in the zipped version of ids and ids shifted over by 1 so that consecutive ids get zipped together\n","        for pair in zip(ids, ids[1:]):\n","            # we increment the frequency of that pair if it is already in the table, otherwise we set it to 1 if its the first instance\n","            counts[pair] = counts.get(pair, 0) + 1\n","            \n","        return counts\n","    \n","    \n","    def merge(self, ids, pair, idx):\n","        \"\"\"\n","        Given a list of ids, a pair of ids, and an id to replace that pair with, merges every pair in the list of ids into idx\n","        \"\"\"\n","        \n","        # our new list of merged ids\n","        newids = []\n","        # a counter variable to help us traverse through the list of ids\n","        i = 0\n","        \n","        # while we have not gone through all of the ids\n","        while i < len(ids):\n","            \n","            # if the current id is the very last id, it cannot be merged so we ignore this if branch and just append it\n","            # however, if the first element of the pair is equal to the current id\n","            #          and if the second element of the pair is equal to the following id\n","            #              then we should merge this pair into our new ids by appending the idx to replace the pair and going forward in ids by 2\n","            if i < len(ids) - 1 and pair[0] == ids[i] and pair[1] == ids[i + 1]:\n","                newids.append(idx)\n","                i += 2\n","                \n","            # otherwise we should just add the id we are on as it is without merging because it is not the pair we want to merge\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","                \n","        return newids\n","    \n","    \n","    def train(self, text, vocab_size, verbose = False):\n","        \"\"\"\n","        Trains the tokenizer on a given text for a given number of iterations (vocab_size) and prints the merges if requested\n","        \"\"\"\n","        \n","        # uses the regex pattern to break the text into chunks\n","        chunks = re.findall(self.compiled_pattern, text)\n","        \n","        # then we assemble our list of ids by encoding each chunk in chunks and making a mini list of ids for each chunk\n","        ids = [list(chunk.encode(\"utf-8\")) for chunk in chunks]\n","        \n","        # while we have not reached the target vocab_size\n","        while self.vocab_size < vocab_size:\n","            \n","            # we want to calculate the current stats on the ids to find which pair we should turn into a token next\n","            stats = {}\n","            # for each chunk of ids in ids\n","            for chunk in ids:\n","                # we want to use the stats we are building up for the entire list of ids\n","                # we want to add on the stats for each individual chunk to get the stats across all chunks\n","                self.get_stats(chunk, stats)\n","            \n","            # we use stats.get as our key which allows us to compare each pairing option in stats by their frequencies and returns to us the pair with the highest frequency\n","            pair = max(stats, key = stats.get)\n","            \n","            # once we have the pair we want, we go through each chunk in ids and merge the pairs into our new token\n","            ids = [self.merge(chunk, pair, self.vocab_size) for chunk in ids]\n","            # then we add this pair into our merges dictionary under its id\n","            self.merges[pair] = self.vocab_size\n","            # then we add this id under our vocabulary as the combination of the two tokens in the pair\n","            self.vocab[self.vocab_size] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","            \n","            # if verbose\n","            if verbose is True:\n","                # print what tokens we are merging into what id\n","                print(f\"merging {self.vocab[pair[0]], self.vocab[pair[1]]} -> {self.vocab_size}\")\n","                \n","            # increase our vocab size by 1\n","            self.vocab_size += 1\n","                \n","    \n","    def encode(self, text):\n","        \"\"\"\n","        Given text, encodes it into tokens based on what our tokenizer has learned from its training\n","        \"\"\"\n","        \n","        # first we encode the text through utf-8 and turn it into a list of integers\n","        ids = list(text.encode(\"utf-8\"))\n","        \n","        # if the length of the text is less than 2, it is already full merged so we should not do anything\n","        while len(text) >= 2:\n","            \n","            # get the stats for this list of ids\n","            stats = self.get_stats(ids)\n","            \n","            # then we choose the pair that occurs the least because in order to build up our larger tokens (e.g. \"Hello\"),\n","            # we first need to build up our smaller tokens (e.g \"He\", \"ll\", \"o\") that make up this larger token\n","            # we key into stats through a lambda expression that gets the frequencies recorded in stats, making sure to set\n","            # pairs that are not in our merges list should be set as infinity so that we do not try to merge pairs that we\n","            # do not actually have an id for\n","            pair = min(stats, key = lambda p: self.merges.get(p, float(\"inf\")))\n","            \n","            # if we run out of possible merges, we will by default eventually choose a pair of infinite frequency so we have\n","            # to add this extra check to break out of the loop if we have no more merges left to make\n","            if pair not in self.merges:\n","                break\n","                \n","            # if we have selected a pair we can merge, then we will merge those pairs in our list of ids under their tokenized id\n","            ids = self.merge(ids, pair, self.merges[pair])\n","            \n","        return ids\n","    \n","    \n","    def decode(self, ids):\n","        \"\"\"\n","        Given a list of ids, decodes them back into plain text\n","        \"\"\"\n","        \n","        # takes each index in the list of ids and gets their bytes in our vocabulary dictionary then joins all the bytes together\n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        \n","        # then we decode these bytes back into text via utf-8, making sure to replace an errors along the way with the ? character\n","        text = text_bytes.decode(\"utf-8\", errors = \"replace\")\n","        \n","        return text"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.839594Z","iopub.status.busy":"2024-03-11T00:14:56.838895Z","iopub.status.idle":"2024-03-11T00:14:56.854323Z","shell.execute_reply":"2024-03-11T00:14:56.853347Z","shell.execute_reply.started":"2024-03-11T00:14:56.839564Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    \"\"\"\n","    Feed forward layer used after each MultiHeadAttention layer\n","    \"\"\"\n","    \n","    \n","    def __init__(self):\n","        super(FeedForward, self).__init__()\n","        self.c_fc = nn.Linear(n_hidden, 4 * n_hidden)\n","        self.gelu = nn.GELU() # apparently GELU is better than RELU here (at least thats what nanoGPT does)\n","        self.c_proj = nn.Linear(4 * n_hidden, n_hidden)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        \n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.855821Z","iopub.status.busy":"2024-03-11T00:14:56.855506Z","iopub.status.idle":"2024-03-11T00:14:56.869705Z","shell.execute_reply":"2024-03-11T00:14:56.868720Z","shell.execute_reply.started":"2024-03-11T00:14:56.855783Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    MultiHeadAttention at the beginning of each block in each decoder layer\n","    \"\"\"\n","    \n","    \n","    def __init__(self):\n","        \n","        super(MultiHeadAttention, self).__init__()\n","        \n","        # combines qkv into one linear layer that will get split into three for efficiency\n","        self.attn = nn.Linear(n_hidden, 3 * n_hidden)\n","        \n","        # our projection after attention\n","        self.proj = nn.Linear(n_hidden, n_hidden)\n","        \n","        # dropout for each stage\n","        self.attn_dropout = nn.Dropout(dropout)\n","        self.proj_dropout = nn.Dropout(dropout)\n","        \n","        # the F.scaled_dot_product_attention is apparently faster and more efficient so use this if available in the PyTorch version\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        \n","        # if not available\n","        if not self.flash:\n","            # just use the regular system where a mask must be registered to block off attention on future tokens\n","            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n","            \n","            \n","    def forward(self, x):\n","        # Batches by Time steps by Channels\n","        B, T, C = x.shape\n","        \n","        # split self.attn into thirds along basically the last dimension of channels\n","        q, k, v = self.attn(x).split(n_hidden, dim = 2) # B, T, C\n","        \n","        # reorganize q, k, v\n","        q = q.view(B, T, n_heads, C // n_heads).transpose(2, 1) # B, H, T, C//H\n","        k = k.view(B, T, n_heads, C // n_heads).transpose(2, 1) # B, H, T, C//H\n","        v = v.view(B, T, n_heads, C // n_heads).transpose(2, 1) # B, H, T, C//H\n","        \n","        # if the speed up function is available\n","        if self.flash:\n","            # simply compute the scaled dot product attention using q, k, v, and if we are not in training mode we should not use dropout\n","            x = F.scaled_dot_product_attention(q, k, v, attn_mask = None, dropout_p = dropout if self.training else 0, is_causal = True)\n","        else:\n","            # queries inner product with k, the reason for the transpose of k is being without the transpose, a row in q corresponds to a row in k\n","            # when we do an inner product, rows in the first get dot producted with cols in the second, so if we want to pair the row in q with the\n","            # row in k we have to transpose k so that a row in q now corresponds to a column in k so that the inner product actually does what is intended\n","            x = (q @ k.transpose(-2, -1)) * k.shape[-1] ** -0.5 # B, H, T, C//H  @  B, H, C//H, T  ->  B, H, T, T\n","            x = x.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\")) # B, H, T, T\n","            x = F.softmax(x, dim = -1) # B, H, T, T\n","            x = self.attn_dropout(x) # B, H, T, T\n","            x = x @ v # B, H, T, T  @  B, H, T, C//H  ->  B, H, T, C//H\n","        \n","        x = x.transpose(1, 2).contiguous().view(B, T, C) # B, H, T, C//H  ->  B, T, H, C//H  ->  B, T, C\n","        x = self.proj_dropout(self.proj(x)) # B, T, C\n","        \n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.871568Z","iopub.status.busy":"2024-03-11T00:14:56.871243Z","iopub.status.idle":"2024-03-11T00:14:56.884220Z","shell.execute_reply":"2024-03-11T00:14:56.883434Z","shell.execute_reply.started":"2024-03-11T00:14:56.871541Z"},"trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","    \"\"\"\n","    The block of MultiHeadAttentions and FeedForwards that make up each layer in n_layers\n","    \"\"\"\n","    \n","    \n","    def __init__(self):\n","        super(Block, self).__init__()\n","        \n","        self.ln1 = nn.LayerNorm(n_hidden)\n","        self.attn = MultiHeadAttention()\n","        self.ln2 = nn.LayerNorm(n_hidden)\n","        self.ffwd = FeedForward()\n","        \n","        \n","    def forward(self, x):\n","        # layer norm the inputs from the previous stage\n","        # use residual connections so that the gradients flow through with more branches resulting in less chance for exploding/vanishing gradients\n","        x = x + self.attn(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.885987Z","iopub.status.busy":"2024-03-11T00:14:56.885678Z","iopub.status.idle":"2024-03-11T00:14:56.905928Z","shell.execute_reply":"2024-03-11T00:14:56.904788Z","shell.execute_reply.started":"2024-03-11T00:14:56.885962Z"},"trusted":true},"outputs":[],"source":["class GPT(nn.Module):\n","    \"\"\"\n","    The full definition of the GPT will all components except the Encoder layer formally included\n","    \"\"\"\n","    \n","    \n","    def __init__(self):\n","        super(GPT, self).__init__()\n","        \n","        self.token_embedding = nn.Embedding(tokenizer.vocab_size, n_hidden)\n","        self.position_embedding = nn.Embedding(block_size, n_hidden) # simple position embeddings instead of the cosine sine thing they use in the original paper\n","        self.dropout = nn.Dropout(dropout)\n","        self.heads = nn.ModuleList([Block() for _ in range(n_layers)]) # this should really be called blocks instead of heads but its ok\n","        self.ln_f = nn.LayerNorm(n_hidden)\n","        self.lm_head = nn.Linear(n_hidden, tokenizer.vocab_size)\n","        \n","        \n","        # weight tying i think for regularization although im not sure because i havent read the paper on it yet\n","        self.token_embedding.weight = self.lm_head.weight\n","        \n","        # apply weight initializations\n","        self.apply(self._init_weights)\n","        \n","        \n","        for pn, p in self.named_parameters():\n","            if pn.endswith(\"c_proj.weight\"):\n","                # not too sure why, maybe its to help with normalization and preventing exploding/vanishing gradients\n","                torch.nn.init.normal_(p, mean = 0.0, std = 0.01 * (2 * n_layers) ** -0.5)\n","           \n","        # visualize how large the model is based on number of parameters\n","        print(sum(p.numel() for p in self.parameters())/1e6, \"million parameters\")\n","        \n","        \n","    def _init_weights(self, module):\n","        \"\"\"\n","        Weight initialization for best results\n","        \"\"\"\n","        \n","        # if nn.Linear\n","        if isinstance(module, nn.Linear):\n","            # initialize weights by drawing from normal distribution centered at 0 with std 0.02\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","            # if there bias is enabled\n","            if module.bias is not None:\n","                # initialize the bias to zeroes\n","                torch.nn.init.zeros_(module.bias)\n","        # if nn.Embedding\n","        elif isinstance(module, nn.Embedding):\n","            # initialize the weights in the same way as nn.Linear\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","        \n","        \n","    def forward(self, idx, targets = None):\n","        \"\"\"\n","        Given a list of indexes, returns the logits probability distribution to draw the next token from.\n","        Optionally argument for targets.\n","        If targets is given, it will calculate the loss and return it.\n","        Otherwise, only the logits will be calculated and the loss will be returned as None.\n","        \"\"\"\n","        # Batches by Time step\n","        B, T = idx.shape\n","        \n","        # get token embedding\n","        tok_emb = self.token_embedding(idx)\n","        \n","        # get position embedding by drawing the embedding vector for each position in the list of indexes given\n","        pos_emb = self.position_embedding(torch.arange(T, device = device))\n","        \n","        # add the token embeddings and the position embeddings and use dropout\n","        x = self.dropout(tok_emb + pos_emb)\n","        \n","        # for each block in the network\n","        for block in self.heads:\n","            # pass the inputs into the block\n","            x = block(x)\n","            \n","        # once past all layers a layer norm will be used\n","        x = self.ln_f(x)\n","        \n","        # if targets are provided\n","        if targets is not None:\n","            # get logits by passing the inputs into the final linear layer\n","            logits = self.lm_head(x)\n","            # then get the loss using cross_entropy and reshaping the logits into B * T, C and the targets into B * T\n","            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index = -1)\n","        # if no targets then no loss can be calculated\n","        else:\n","            # take only the last time step because it is likely we will be generating tokens if no targets are given so we only care about the last time step\n","            # because this is the model's prediction for what token comes next\n","            logits = self.lm_head(x[:, [-1], :])\n","            loss = None\n","            \n","        return logits, loss\n","        \n","        \n","    def generate(self):\n","        \"\"\"\n","        No parameters to give.\n","        A random story is generated from scratch by using the <|startofstory|> token and stops generating when the model predicts a <|endofstory|> token\n","        \"\"\"\n","        \n","        # switch to eval mode so that dropout isnt used\n","        model.eval()\n","        \n","        # create our list of ids containing only the special start token and reshape into B x T (1 x 1)\n","        ids = [tokenizer.special_tokens[\"<|startofstory|>\"]]\n","        ids = torch.tensor(ids).view(1, 1).to(device)\n","    \n","        # we want to just keep generating until the end of story is predicted by the model\n","        while True:\n","            # pass the last block_size tokens into the model\n","            logits, loss = self(ids[:, -block_size:] if ids.shape[-1] > block_size else ids)\n","        \n","            # I think this is actually an unnecessary step because we already get the last time step by not passing any targets to the model\n","            logits = logits[:, -1, :]\n","        \n","            # apply the softmax to get the probability distribution\n","            logits = F.softmax(logits, dim = -1)\n","        \n","            # randomly sample a token from the probability distribution\n","            prediction = torch.multinomial(logits, num_samples = 1, replacement = True)\n","            \n","            # if the sampled token is the special end token, then that means the story is predicted to end here and we break out of the loop\n","            if prediction == (tokenizer.special_tokens[\"<|endofstory|>\"]):\n","                break\n","                \n","            # otherwise we append the newly predicted token to the ids and go to the next iteration of generation\n","            ids = torch.cat((ids, prediction), dim = 1)\n","        \n","        # finally we take our ids and crop the first special start token out, then we move to cpu, then we convert to np array for efficiency, then we decode using our tokenizer into text\n","        return tokenizer.decode(np.array(ids[0][1:].to(\"cpu\")))"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Training the tokenizer on a chunk of the validation text</h1>"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.910523Z","iopub.status.busy":"2024-03-11T00:14:56.909847Z","iopub.status.idle":"2024-03-11T00:14:56.923111Z","shell.execute_reply":"2024-03-11T00:14:56.922182Z","shell.execute_reply.started":"2024-03-11T00:14:56.910491Z"},"trusted":true},"outputs":[],"source":["# initializing the tokenizer using the GPT4 regex pattern\n","tokenizer = Tokenizer(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:56.924671Z","iopub.status.busy":"2024-03-11T00:14:56.924314Z","iopub.status.idle":"2024-03-11T00:14:57.001272Z","shell.execute_reply":"2024-03-11T00:14:57.000426Z","shell.execute_reply.started":"2024-03-11T00:14:56.924639Z"},"trusted":true},"outputs":[],"source":["tokenizer_text = \"\" # text for training the tokenizer\n","\n","# for each story in the validation set\n","for story in val_text:\n","    # add the story to the tokenizer text\n","    tokenizer_text += story\n","    \n","# take only 1/32 th of the tokenizer text because the amount of text in this dataset is just too much and this small chunk is representative enough of the entire dataset\n","tokenizer_text = tokenizer_text[:len(tokenizer_text) // 32]"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:14:57.002808Z","iopub.status.busy":"2024-03-11T00:14:57.002497Z","iopub.status.idle":"2024-03-11T00:14:57.009465Z","shell.execute_reply":"2024-03-11T00:14:57.008426Z","shell.execute_reply.started":"2024-03-11T00:14:57.002782Z"},"trusted":true},"outputs":[{"data":{"text/plain":["599697"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# this small of a chunk is still 600k characters\n","len(tokenizer_text)"]},{"cell_type":"code","execution_count":13,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-11T00:14:57.011275Z","iopub.status.busy":"2024-03-11T00:14:57.010599Z","iopub.status.idle":"2024-03-11T00:21:16.720616Z","shell.execute_reply":"2024-03-11T00:21:16.719562Z","shell.execute_reply.started":"2024-03-11T00:14:57.011240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["merging (b'h', b'e') -> 256\n","merging (b' ', b't') -> 257\n","merging (b' ', b'a') -> 258\n","merging (b' ', b's') -> 259\n","merging (b' ', b'w') -> 260\n","merging (b'n', b'd') -> 261\n","merging (b' t', b'he') -> 262\n","merging (b'e', b'd') -> 263\n","merging (b' t', b'o') -> 264\n","merging (b' a', b'nd') -> 265\n","merging (b' ', b'b') -> 266\n","merging (b'i', b'n') -> 267\n","merging (b' ', b'h') -> 268\n","merging (b' w', b'a') -> 269\n","merging (b'i', b't') -> 270\n","merging (b'r', b'e') -> 271\n","merging (b' ', b'f') -> 272\n","merging (b'o', b'u') -> 273\n","merging (b' ', b'he') -> 274\n","merging (b' ', b'l') -> 275\n","merging (b' ', b'c') -> 276\n","merging (b' ', b'd') -> 277\n","merging (b'e', b'r') -> 278\n","merging (b' wa', b's') -> 279\n","merging (b' ', b'm') -> 280\n","merging (b' ', b'p') -> 281\n","merging (b'o', b'n') -> 282\n","merging (b'a', b'y') -> 283\n","merging (b'o', b'm') -> 284\n","merging (b' ', b'T') -> 285\n","merging (b'a', b'r') -> 286\n","merging (b'\\n', b'\\n') -> 287\n","merging (b'i', b'l') -> 288\n","merging (b'a', b't') -> 289\n","merging (b'in', b'g') -> 290\n","merging (b'i', b's') -> 291\n","merging (b'i', b'm') -> 292\n","merging (b' ', b'g') -> 293\n","merging (b'i', b'd') -> 294\n","merging (b' s', b'a') -> 295\n","merging (b' ', b'n') -> 296\n","merging (b'e', b'n') -> 297\n","merging (b'o', b'r') -> 298\n","merging (b' h', b'a') -> 299\n","merging (b' ', b'it') -> 300\n","merging (b'a', b'n') -> 301\n","merging (b' ', b'S') -> 302\n","merging (b' t', b'h') -> 303\n","merging (b' ', b'o') -> 304\n","merging (b'l', b'l') -> 305\n","merging (b' he', b'r') -> 306\n","merging (b'l', b'e') -> 307\n","merging (b' T', b'he') -> 308\n","merging (b'o', b't') -> 309\n","merging (b'e', b't') -> 310\n","merging (b' ', b'H') -> 311\n","merging (b'il', b'y') -> 312\n","merging (b'.', b'\\n\\n') -> 313\n","merging (b'u', b't') -> 314\n","merging (b'a', b'm') -> 315\n","merging (b' ', b'u') -> 316\n","merging (b'i', b'r') -> 317\n","merging (b' ', b'in') -> 318\n","merging (b'v', b'er') -> 319\n","merging (b'L', b'ily') -> 320\n","merging (b' ', b'e') -> 321\n","merging (b'c', b'e') -> 322\n","merging (b'l', b'd') -> 323\n","merging (b' S', b'he') -> 324\n","merging (b'O', b'n') -> 325\n","merging (b' H', b'e') -> 326\n","merging (b'o', b'w') -> 327\n","merging (b' b', b'e') -> 328\n","merging (b'c', b'k') -> 329\n","merging (b'm', b'y') -> 330\n","merging (b'r', b'i') -> 331\n","merging (b' h', b'is') -> 332\n","merging (b' s', b'he') -> 333\n","merging (b' s', b't') -> 334\n","merging (b' sa', b'id') -> 335\n","merging (b'k', b'e') -> 336\n","merging (b' d', b'ay') -> 337\n","merging (b'it', b'h') -> 338\n","merging (b' ', b'Lily') -> 339\n","merging (b' ', b'\"') -> 340\n","merging (b' w', b'ith') -> 341\n","merging (b' ', b'y') -> 342\n","merging (b' l', b'o') -> 343\n","merging (b' ', b'on') -> 344\n","merging (b'v', b'e') -> 345\n","merging (b' o', b'f') -> 346\n","merging (b' p', b'l') -> 347\n","merging (b' th', b'at') -> 348\n","merging (b'p', b'p') -> 349\n","merging (b'i', b'g') -> 350\n","merging (b' u', b'p') -> 351\n","merging (b' s', b'o') -> 352\n","merging (b's', b't') -> 353\n","merging (b' ', b'r') -> 354\n","merging (b'ver', b'y') -> 355\n","merging (b' m', b'om') -> 356\n","merging (b'k', b'ed') -> 357\n","merging (b' pl', b'ay') -> 358\n","merging (b'n', b't') -> 359\n","merging (b' ', b'I') -> 360\n","merging (b'e', b'l') -> 361\n","merging (b' The', b'y') -> 362\n","merging (b\"'\", b's') -> 363\n","merging (b'it', b't') -> 364\n","merging (b'a', b'd') -> 365\n","merging (b'im', b'e') -> 366\n","merging (b' ha', b'd') -> 367\n","merging (b' y', b'ou') -> 368\n","merging (b'en', b't') -> 369\n","merging (b' the', b'y') -> 370\n","merging (b' ', b'\\n\\n') -> 371\n","merging (b' t', b'ime') -> 372\n","merging (b' f', b'or') -> 373\n","merging (b'ou', b'ld') -> 374\n","merging (b'itt', b'le') -> 375\n","merging (b' l', b'ittle') -> 376\n","merging (b' the', b're') -> 377\n","merging (b'e', b's') -> 378\n","merging (b' T', b'im') -> 379\n","merging (b' w', b'e') -> 380\n","merging (b'he', b'r') -> 381\n","merging (b'c', b'h') -> 382\n","merging (b' ha', b'pp') -> 383\n","merging (b'l', b'y') -> 384\n","merging (b'ou', b't') -> 385\n","merging (b'u', b'n') -> 386\n","merging (b' ', b'B') -> 387\n","merging (b's', b'e') -> 388\n","merging (b' ', b'k') -> 389\n","merging (b'On', b'ce') -> 390\n","merging (b'e', b'nd') -> 391\n","merging (b' b', b'ut') -> 392\n","merging (b'v', b'ed') -> 393\n","merging (b'ou', b'nd') -> 394\n","merging (b'On', b'e') -> 395\n","merging (b'om', b'e') -> 396\n","merging (b\"'\", b't') -> 397\n","merging (b'T', b'he') -> 398\n","merging (b' w', b'h') -> 399\n","merging (b'o', b'o') -> 400\n","merging (b'a', b'll') -> 401\n","merging (b' wa', b'nt') -> 402\n","merging (b' b', b'ig') -> 403\n","merging (b'id', b'e') -> 404\n","merging (b' ', b'very') -> 405\n","merging (b' s', b'h') -> 406\n","merging (b' d', b'o') -> 407\n","merging (b'a', b'l') -> 408\n","merging (b' Tim', b'my') -> 409\n","merging (b' up', b'on') -> 410\n","merging (b' f', b'ri') -> 411\n","merging (b'a', b'ke') -> 412\n","merging (b' n', b'e') -> 413\n","merging (b' fri', b'end') -> 414\n","merging (b'h', b't') -> 415\n","merging (b' l', b'i') -> 416\n","merging (b' happ', b'y') -> 417\n","merging (b' ', b're') -> 418\n","merging (b' sa', b'w') -> 419\n","merging (b' a', b'n') -> 420\n","merging (b' a', b's') -> 421\n","merging (b'a', b'ck') -> 422\n","merging (b't', b'er') -> 423\n","merging (b'.', b'Once') -> 424\n","merging (b' h', b'im') -> 425\n","merging (b' n', b'am') -> 426\n","merging (b'r', b'y') -> 427\n","merging (b' n', b'ot') -> 428\n","merging (b' ', b'M') -> 429\n","merging (b'.', b'\"') -> 430\n","merging (b'ir', b'l') -> 431\n","merging (b'r', b'a') -> 432\n","merging (b' g', b'irl') -> 433\n","merging (b' ', b'j') -> 434\n","merging (b' nam', b'ed') -> 435\n","merging (b' s', b'm') -> 436\n","merging (b' b', b'o') -> 437\n","merging (b'o', b're') -> 438\n","merging (b'he', b'n') -> 439\n","merging (b'ar', b'd') -> 440\n","merging (b'w', b'ay') -> 441\n","merging (b' ', b'out') -> 442\n","merging (b' w', b'ent') -> 443\n","merging (b' we', b're') -> 444\n","merging (b' to', b'o') -> 445\n","merging (b' g', b'o') -> 446\n","merging (b'u', b'g') -> 447\n","merging (b' lo', b'ved') -> 448\n","merging (b' lo', b'o') -> 449\n","merging (b' I', b't') -> 450\n","merging (b' c', b'ould') -> 451\n","merging (b'e', b'c') -> 452\n","merging (b'l', b'p') -> 453\n","merging (b' d', b'id') -> 454\n","merging (b' he', b'lp') -> 455\n","merging (b' want', b'ed') -> 456\n","merging (b' ', b'J') -> 457\n","merging (b' ', b'One') -> 458\n","merging (b'a', b'in') -> 459\n","merging (b'u', b'l') -> 460\n","merging (b'!', b'\"') -> 461\n","merging (b' a', b'll') -> 462\n","merging (b'i', b'c') -> 463\n","merging (b'r', b'om') -> 464\n","merging (b' ', b'is') -> 465\n","merging (b'u', b'r') -> 466\n","merging (b' a', b't') -> 467\n","merging (b'ar', b't') -> 468\n","merging (b' k', b'n') -> 469\n","merging (b'il', b'l') -> 470\n","merging (b' s', b'p') -> 471\n","merging (b's', b'ide') -> 472\n","merging (b'ar', b'k') -> 473\n","merging (b'i', b'nd') -> 474\n","merging (b' s', b'e') -> 475\n","merging (b'on', b'e') -> 476\n","merging (b' e', b'x') -> 477\n","merging (b'i', b'f') -> 478\n","merging (b' a', b'l') -> 479\n","merging (b' ', b'A') -> 480\n","merging (b'r', b'ound') -> 481\n","merging (b' c', b'a') -> 482\n","merging (b'el', b't') -> 483\n","merging (b'u', b're') -> 484\n","merging (b' b', b'ack') -> 485\n","merging (b' the', b'm') -> 486\n","merging (b'am', b'e') -> 487\n","merging (b'u', b'm') -> 488\n","merging (b' f', b'elt') -> 489\n","merging (b' friend', b's') -> 490\n","merging (b'g', b'ht') -> 491\n","merging (b' sm', b'il') -> 492\n","merging (b'he', b'd') -> 493\n","merging (b' as', b'ked') -> 494\n","merging (b' f', b'e') -> 495\n","merging (b'g', b'et') -> 496\n","merging (b'h', b'ing') -> 497\n","merging (b'i', b'ce') -> 498\n","merging (b'oo', b'd') -> 499\n","merging (b' c', b'l') -> 500\n","merging (b' bo', b'y') -> 501\n","merging (b' s', b'c') -> 502\n","merging (b' the', b'ir') -> 503\n","merging (b' m', b'an') -> 504\n","merging (b' to', b'y') -> 505\n","merging (b' l', b'e') -> 506\n","merging (b'on', b'g') -> 507\n","merging (b' w', b'ould') -> 508\n","merging (b'?', b'\"') -> 509\n","merging (b' ha', b've') -> 510\n","merging (b's', b's') -> 511\n","merging (b' m', b'ake') -> 512\n","merging (b'get', b'her') -> 513\n","merging (b' n', b'o') -> 514\n","merging (b' sa', b'd') -> 515\n","merging (b' c', b'an') -> 516\n","merging (b' st', b'art') -> 517\n","merging (b' f', b'un') -> 518\n","merging (b' s', b'ome') -> 519\n","merging (b'f', b'ul') -> 520\n","merging (b' start', b'ed') -> 521\n","merging (b' B', b'ut') -> 522\n","merging (b' mom', b'my') -> 523\n","merging (b' w', b'hen') -> 524\n","merging (b'd', b'd') -> 525\n","merging (b' to', b'gether') -> 526\n","merging (b' r', b'o') -> 527\n","merging (b' a', b'round') -> 528\n","merging (b' be', b'c') -> 529\n","merging (b' g', b'ot') -> 530\n","merging (b' loo', b'ked') -> 531\n","merging (b'k', b'ing') -> 532\n","merging (b'i', b'ck') -> 533\n","merging (b' smil', b'ed') -> 534\n","merging (b'r', b'o') -> 535\n","merging (b' did', b'n') -> 536\n","merging (b'i', b'e') -> 537\n","merging (b' a', b'way') -> 538\n","merging (b' li', b'ke') -> 539\n","merging (b' f', b'ound') -> 540\n","merging (b' m', b'e') -> 541\n","merging (b' a', b're') -> 542\n","merging (b' p', b'ark') -> 543\n","merging (b'ir', b'd') -> 544\n","merging (b' a', b'g') -> 545\n","merging (b' p', b'ut') -> 546\n","merging (b' h', b'ome') -> 547\n","merging (b' s', b'w') -> 548\n","merging (b' se', b'e') -> 549\n","merging (b'.\"', b'\\n\\n') -> 550\n","merging (b' w', b'or') -> 551\n","merging (b'ar', b'ed') -> 552\n","merging (b'ow', b'n') -> 553\n","merging (b'p', b'l') -> 554\n","merging (b'ad', b'e') -> 555\n","merging (b' ', b'W') -> 556\n","merging (b' wh', b'at') -> 557\n","merging (b' m', b'u') -> 558\n","merging (b'b', b'le') -> 559\n","merging (b' b', b'ird') -> 560\n","merging (b'o', b'p') -> 561\n","merging (b'om', b'et') -> 562\n","merging (b' r', b'an') -> 563\n","merging (b' B', b'en') -> 564\n","merging (b'\\xc3', b'\\xa2') -> 565\n","merging (b'\\xe2', b'\\x82') -> 566\n","merging (b'\\xe2\\x82', b'\\xac') -> 567\n","merging (b' d', b'ec') -> 568\n","merging (b'ri', b'ed') -> 569\n","merging (b' f', b'a') -> 570\n","merging (b'ing', b's') -> 571\n","merging (b' c', b'o') -> 572\n","merging (b',', b'\"') -> 573\n","merging (b' e', b'very') -> 574\n","merging (b' c', b'ame') -> 575\n","merging (b'a', b've') -> 576\n","merging (b'a', b'u') -> 577\n","merging (b' m', b'ade') -> 578\n","merging (b'it', b'ed') -> 579\n","merging (b' s', b'omet') -> 580\n","merging (b'e', b'p') -> 581\n","merging (b' ag', b'ain') -> 582\n","merging (b'a', b'x') -> 583\n","merging (b'n', b'y') -> 584\n","merging (b'g', b'e') -> 585\n","merging (b'r', b'ou') -> 586\n","merging (b'all', b'y') -> 587\n","merging (b'id', b'ed') -> 588\n","merging (b' fe', b'el') -> 589\n","merging (b' dec', b'ided') -> 590\n","merging (b' wh', b'o') -> 591\n","merging (b' ex', b'c') -> 592\n","merging (b'way', b's') -> 593\n","merging (b' out', b'side') -> 594\n","merging (b' al', b'ways') -> 595\n","merging (b' t', b're') -> 596\n","merging (b' ', b'E') -> 597\n","merging (b' f', b'rom') -> 598\n","merging (b'au', b'se') -> 599\n","merging (b'h', b'at') -> 600\n","merging (b' play', b'ing') -> 601\n","merging (b'Y', b'ou') -> 602\n","merging (b't', b'her') -> 603\n","merging (b' the', b'n') -> 604\n","merging (b'ig', b'ht') -> 605\n","merging (b' bec', b'ause') -> 606\n","merging (b' wa', b'l') -> 607\n","merging (b'a', b's') -> 608\n","merging (b' d', b'own') -> 609\n","merging (b' g', b'et') -> 610\n","merging (b' ', b'F') -> 611\n","merging (b' o', b'ther') -> 612\n","merging (b'u', b'st') -> 613\n","merging (b'u', b'e') -> 614\n","merging (b' somet', b'hing') -> 615\n","merging (b' M', b'ax') -> 616\n","merging (b' p', b'o') -> 617\n","merging (b' f', b'l') -> 618\n","merging (b' c', b'ar') -> 619\n","merging (b' T', b'om') -> 620\n","merging (b' l', b'a') -> 621\n","merging (b'u', b'dd') -> 622\n","merging (b' exc', b'ited') -> 623\n","merging (b' e', b'at') -> 624\n","merging (b'oo', b'k') -> 625\n","merging (b' t', b'a') -> 626\n","merging (b'ou', b's') -> 627\n","merging (b'ou', b'ght') -> 628\n","merging (b'an', b't') -> 629\n","merging (b' ca', b're') -> 630\n","merging (b' sc', b'ared') -> 631\n","merging (b' kn', b'e') -> 632\n","merging (b'ou', b'se') -> 633\n","merging (b'pp', b'ed') -> 634\n","merging (b'it', b'e') -> 635\n","merging (b' ', b'v') -> 636\n","merging (b' on', b'e') -> 637\n","merging (b' g', b'ood') -> 638\n","merging (b' o', b'ld') -> 639\n","merging (b' ', b'ke') -> 640\n","merging (b' l', b'ot') -> 641\n","merging (b' kn', b'ow') -> 642\n","merging (b'ot', b'her') -> 643\n","merging (b' c', b'at') -> 644\n","merging (b' ne', b'w') -> 645\n","merging (b' to', b'ld') -> 646\n","merging (b' th', b'ings') -> 647\n","merging (b'f', b'ter') -> 648\n","merging (b'u', b's') -> 649\n","merging (b' f', b'ind') -> 650\n","merging (b' you', b'r') -> 651\n","merging (b'a', b'ch') -> 652\n","merging (b'd', b'y') -> 653\n","merging (b'e', b'll') -> 654\n","merging (b' a', b'b') -> 655\n","merging (b' too', b'k') -> 656\n","merging (b'u', b'ck') -> 657\n","merging (b' mu', b'ch') -> 658\n","merging (b' in', b'side') -> 659\n","merging (b'an', b'k') -> 660\n","merging (b'ar', b'n') -> 661\n","merging (b' do', b'g') -> 662\n","merging (b' t', b'ake') -> 663\n","merging (b' g', b'ra') -> 664\n","merging (b'ar', b'a') -> 665\n","merging (b'f', b'e') -> 666\n","merging (b' c', b'h') -> 667\n","merging (b' ab', b'out') -> 668\n","merging (b' kne', b'w') -> 669\n","merging (b'i', b've') -> 670\n","merging (b' ', b'if') -> 671\n","merging (b' th', b'ought') -> 672\n","merging (b' h', b'ow') -> 673\n","merging (b'udd', b'en') -> 674\n","merging (b' li', b'ked') -> 675\n","merging (b' h', b'ug') -> 676\n","merging (b'udden', b'ly') -> 677\n","merging (b' m', b'ore') -> 678\n","merging (b' an', b'y') -> 679\n","merging (b' s', b'l') -> 680\n","merging (b' g', b'ave') -> 681\n","merging (b' o', b'ver') -> 682\n","merging (b' s', b'un') -> 683\n","merging (b' S', b'am') -> 684\n","merging (b' s', b'ay') -> 685\n","merging (b' t', b'ried') -> 686\n","merging (b' could', b'n') -> 687\n","merging (b'i', b'on') -> 688\n","merging (b't', b'o') -> 689\n","merging (b' t', b'r') -> 690\n","merging (b' toy', b's') -> 691\n","merging (b' h', b'ouse') -> 692\n","merging (b' m', b'y') -> 693\n","merging (b'er', b's') -> 694\n","merging (b' of', b'f') -> 695\n","merging (b'il', b'e') -> 696\n","merging (b'et', b'ter') -> 697\n","merging (b're', b'at') -> 698\n","merging (b' ', b'You') -> 699\n","merging (b' play', b'ed') -> 700\n","merging (b' b', b'r') -> 701\n","merging (b' loo', b'k') -> 702\n","merging (b'n', b'a') -> 703\n","merging (b're', b't') -> 704\n","merging (b' ', b'im') -> 705\n","merging (b' d', b'ad') -> 706\n","merging (b' le', b'arn') -> 707\n","merging (b' ', b'D') -> 708\n","merging (b'ou', b'r') -> 709\n","merging (b'is', b'e') -> 710\n","merging (b'c', b'hed') -> 711\n","merging (b't', b'y') -> 712\n","merging (b'en', b'ed') -> 713\n","merging (b'nd', b'er') -> 714\n","merging (b'b', b'b') -> 715\n","merging (b'q', b'u') -> 716\n","merging (b'c', b't') -> 717\n","merging (b'es', b's') -> 718\n","merging (b' j', b'ust') -> 719\n","merging (b'ur', b't') -> 720\n","merging (b'i', b'z') -> 721\n","merging (b' p', b'a') -> 722\n","merging (b' M', b'om') -> 723\n","merging (b' H', b'er') -> 724\n","merging (b' c', b'om') -> 725\n","merging (b'B', b'ut') -> 726\n","merging (b'g', b'ed') -> 727\n","merging (b'or', b't') -> 728\n","merging (b'v', b'en') -> 729\n","merging (b' b', b'all') -> 730\n","merging (b' st', b'r') -> 731\n","merging (b' sh', b'ow') -> 732\n","merging (b'is', b't') -> 733\n","merging (b's', b'el') -> 734\n","merging (b' w', b'ill') -> 735\n","merging (b'u', b'll') -> 736\n","merging (b' p', b'rou') -> 737\n","merging (b' prou', b'd') -> 738\n","merging (b' th', b'an') -> 739\n","merging (b' E', b'very') -> 740\n","merging (b' e', b'nd') -> 741\n","merging (b' b', b'etter') -> 742\n","merging (b' he', b'ard') -> 743\n","merging (b' an', b'im') -> 744\n","merging (b' anim', b'al') -> 745\n","merging (b' b', b'l') -> 746\n","merging (b' im', b'p') -> 747\n","merging (b' b', b'u') -> 748\n","merging (b' b', b'ot') -> 749\n","merging (b'ec', b'i') -> 750\n","merging (b're', b'am') -> 751\n","merging (b'e', b'm') -> 752\n","merging (b'in', b'k') -> 753\n","merging (b' J', b'o') -> 754\n","merging (b' p', b'ick') -> 755\n","merging (b' ne', b'ver') -> 756\n","merging (b' ', b'L') -> 757\n","merging (b' p', b'e') -> 758\n","merging (b'ri', b'ght') -> 759\n","merging (b'i', b'ed') -> 760\n","merging (b'at', b'e') -> 761\n","merging (b' r', b'un') -> 762\n","merging (b' ', b'or') -> 763\n","merging (b'sel', b'f') -> 764\n","merging (b\"'\", b'm') -> 765\n","merging (b' F', b'rom') -> 766\n","merging (b' st', b'ay') -> 767\n","merging (b'h', b'n') -> 768\n","merging (b'o', b's') -> 769\n","merging (b' p', b'ret') -> 770\n","merging (b' in', b'to') -> 771\n","merging (b' l', b'ong') -> 772\n","merging (b' c', b'he') -> 773\n","merging (b' sp', b'eci') -> 774\n","merging (b' speci', b'al') -> 775\n","merging (b'b', b'er') -> 776\n","merging (b' la', b'ug') -> 777\n","merging (b' ne', b'ed') -> 778\n","merging (b' t', b'e') -> 779\n","merging (b' th', b'is') -> 780\n","merging (b' o', b'p') -> 781\n","merging (b' a', b'd') -> 782\n","merging (b' u', b'nt') -> 783\n","merging (b' t', b'ry') -> 784\n","merging (b' h', b'urt') -> 785\n","merging (b'!\"', b'\\n\\n') -> 786\n","merging (b'v', b'ent') -> 787\n","merging (b'he', b're') -> 788\n","merging (b'T', b'h') -> 789\n","merging (b'um', b'p') -> 790\n","merging (b'f', b't') -> 791\n","merging (b'in', b'e') -> 792\n","merging (b'm', b'a') -> 793\n","merging (b' wa', b't') -> 794\n","merging (b' care', b'ful') -> 795\n","merging (b' ex', b'pl') -> 796\n","merging (b' s', b'n') -> 797\n","merging (b' S', b'ara') -> 798\n","merging (b' re', b'pl') -> 799\n","merging (b' unt', b'il') -> 800\n","merging (b' learn', b'ed') -> 801\n","merging (b' k', b'ind') -> 802\n","merging (b' t', b'w') -> 803\n","merging (b' g', b'ard') -> 804\n","merging (b' gard', b'en') -> 805\n","merging (b' wa', b'y') -> 806\n","merging (b'b', b'y') -> 807\n","merging (b'M', b'om') -> 808\n","merging (b'e', b'et') -> 809\n","merging (b'ad', b'y') -> 810\n","merging (b' f', b'o') -> 811\n","merging (b'c', b'y') -> 812\n","merging (b' tre', b'e') -> 813\n","merging (b' s', b'k') -> 814\n","merging (b' lot', b's') -> 815\n","merging (b' c', b'le') -> 816\n","merging (b' feel', b'ing') -> 817\n","merging (b' n', b'ice') -> 818\n","merging (b' ', b'en') -> 819\n","merging (b' d', b'on') -> 820\n","merging (b' wa', b'ter') -> 821\n","merging (b'a', b'g') -> 822\n","merging (b' j', b'o') -> 823\n","merging (b' repl', b'ied') -> 824\n","merging (b'p', b't') -> 825\n","merging (b' c', b'u') -> 826\n","merging (b' animal', b's') -> 827\n","merging (b' H', b'is') -> 828\n","merging (b' h', b'ard') -> 829\n","merging (b'a', b'ce') -> 830\n","merging (b'g', b'ry') -> 831\n","merging (b' J', b'ack') -> 832\n","merging (b'n', b'na') -> 833\n","merging (b' sm', b'all') -> 834\n","merging (b' st', b'or') -> 835\n","merging (b' S', b'o') -> 836\n","merging (b' ha', b'nd') -> 837\n","merging (b' st', b'ill') -> 838\n","merging (b'an', b'e') -> 839\n","merging (b'The', b'y') -> 840\n","merging (b'or', b'ry') -> 841\n","merging (b' sa', b'fe') -> 842\n","merging (b' ro', b'om') -> 843\n","merging (b'il', b'ly') -> 844\n","merging (b' e', b'ven') -> 845\n","merging (b' f', b'ar') -> 846\n","merging (b' ', b'C') -> 847\n","merging (b' s', b'orry') -> 848\n","merging (b' m', b'o') -> 849\n","merging (b'n', b'ing') -> 850\n","merging (b' be', b'ar') -> 851\n","merging (b'T', b'im') -> 852\n","merging (b' re', b'm') -> 853\n","merging (b' b', b'y') -> 854\n","merging (b' c', b'all') -> 855\n","merging (b'v', b'ing') -> 856\n","merging (b'o', b'pped') -> 857\n","merging (b' bot', b'h') -> 858\n","merging (b'g', b'er') -> 859\n","merging (b'a', b're') -> 860\n","merging (b' co', b'l') -> 861\n","merging (b' f', b'i') -> 862\n","merging (b'iz', b'ed') -> 863\n","merging (b' hug', b'ged') -> 864\n","merging (b' l', b'et') -> 865\n","merging (b' wh', b'ile') -> 866\n","merging (b' p', b'ic') -> 867\n","merging (b' but', b'ter') -> 868\n","merging (b'H', b'e') -> 869\n","merging (b' ro', b'ck') -> 870\n","merging (b' W', b'hen') -> 871\n","merging (b' pick', b'ed') -> 872\n","merging (b'ou', b'd') -> 873\n","merging (b' W', b'e') -> 874\n","merging (b'?\"', b'\\n\\n') -> 875\n","merging (b'a', b'nd') -> 876\n","merging (b' c', b'on') -> 877\n","merging (b'ig', b'h') -> 878\n","merging (b'W', b'hen') -> 879\n","merging (b'il', b'd') -> 880\n","merging (b' tw', b'o') -> 881\n","merging (b'n', b'er') -> 882\n","merging (b' lo', b've') -> 883\n","merging (b' ke', b'pt') -> 884\n","merging (b're', b'ss') -> 885\n","merging (b'I', b't') -> 886\n","merging (b' A', b'nd') -> 887\n","merging (b' pret', b'ty') -> 888\n","merging (b' fa', b'st') -> 889\n","merging (b'x', b't') -> 890\n","merging (b' f', b'in') -> 891\n","merging (b'a', b'se') -> 892\n","merging (b' s', b'ur') -> 893\n","merging (b' imp', b'ort') -> 894\n","merging (b' r', b'ain') -> 895\n","merging (b' br', b'other') -> 896\n","merging (b' Sara', b'h') -> 897\n","merging (b' do', b'll') -> 898\n","merging (b'em', b'ber') -> 899\n","merging (b'ra', b've') -> 900\n","merging (b' be', b'a') -> 901\n","merging (b'o', b'l') -> 902\n","merging (b'S', b'uddenly') -> 903\n","merging (b'f', b'ore') -> 904\n","merging (b' u', b'nder') -> 905\n","merging (b' bo', b'x') -> 906\n","merging (b'L', b'et') -> 907\n","merging (b' o', b'w') -> 908\n","merging (b' a', b'r') -> 909\n","merging (b' ', b'\\xc3\\xa2') -> 910\n","merging (b'\\xe2\\x82\\xac', b'\\xc5') -> 911\n","merging (b'\\xe2\\x82\\xac\\xc5', b'\\x93') -> 912\n","merging (b'p', b'ot') -> 913\n","merging (b' ', b'ide') -> 914\n","merging (b'm', b'er') -> 915\n","merging (b' sk', b'y') -> 916\n","merging (b'is', b's') -> 917\n","merging (b' u', b's') -> 918\n","merging (b' sh', b'in') -> 919\n","merging (b' li', b'ved') -> 920\n","merging (b' be', b'fore') -> 921\n","merging (b' ide', b'a') -> 922\n","merging (b' ke', b'ep') -> 923\n","merging (b' import', b'ant') -> 924\n","merging (b'i', b'es') -> 925\n","merging (b' cle', b'an') -> 926\n","merging (b'is', b'h') -> 927\n","merging (b' laug', b'hed') -> 928\n","merging (b' f', b'am') -> 929\n","merging (b'ic', b'ed') -> 930\n","merging (b't', b'ed') -> 931\n","merging (b' al', b's') -> 932\n","merging (b' als', b'o') -> 933\n","merging (b' d', b'an') -> 934\n","merging (b' ne', b'xt') -> 935\n","merging (b' s', b'ure') -> 936\n","merging (b' rem', b'ember') -> 937\n","merging (b' y', b'um') -> 938\n","merging (b't', b'h') -> 939\n","merging (b'oo', b'l') -> 940\n","merging (b' f', b'ly') -> 941\n","merging (b'he', b's') -> 942\n","merging (b' fl', b'ow') -> 943\n","merging (b'ar', b's') -> 944\n","merging (b' cl', b'os') -> 945\n","merging (b' The', b'n') -> 946\n","merging (b'i', b'p') -> 947\n","merging (b'u', b'cy') -> 948\n","merging (b'is', b'hed') -> 949\n","merging (b' col', b'or') -> 950\n","merging (b'k', b'ay') -> 951\n","merging (b'g', b'an') -> 952\n","merging (b'l', b'ed') -> 953\n","merging (b' ', b'qu') -> 954\n","merging (b' a', b'c') -> 955\n","merging (b't', b'hing') -> 956\n","merging (b' than', b'ked') -> 957\n","merging (b' l', b'ist') -> 958\n","merging (b' g', b'reat') -> 959\n","merging (b' bea', b'ut') -> 960\n","merging (b' butter', b'f') -> 961\n","merging (b' be', b'st') -> 962\n","merging (b' c', b'ome') -> 963\n","merging (b' g', b'ive') -> 964\n","merging (b' wal', b'ked') -> 965\n","merging (b' j', b'ump') -> 966\n","merging (b' e', b'ach') -> 967\n","merging (b' g', b'r') -> 968\n","merging (b' ad', b'vent') -> 969\n","merging (b' pic', b't') -> 970\n","merging (b' pict', b'ure') -> 971\n","merging (b' b', b'ad') -> 972\n","merging (b' A', b'nna') -> 973\n","merging (b'g', b'h') -> 974\n","merging (b' b', b'ed') -> 975\n","merging (b're', b'e') -> 976\n","merging (b'T', b'hat') -> 977\n","merging (b' wa', b'it') -> 978\n","merging (b' b', b'ook') -> 979\n","merging (b' d', b'el') -> 980\n","merging (b' ', b'z') -> 981\n","merging (b' not', b'iced') -> 982\n","merging (b'Y', b'es') -> 983\n","merging (b' n', b'ow') -> 984\n","merging (b'f', b'f') -> 985\n","merging (b' no', b'dd') -> 986\n","merging (b' butterf', b'ly') -> 987\n","merging (b' p', b'r') -> 988\n","merging (b' go', b'ing') -> 989\n","merging (b' J', b'ane') -> 990\n","merging (b' S', b'pot') -> 991\n","merging (b' cl', b'im') -> 992\n","merging (b'le', b'w') -> 993\n","merging (b' b', b'rave') -> 994\n","merging (b'a', b'ble') -> 995\n","merging (b' advent', b'ure') -> 996\n","merging (b' st', b'opped') -> 997\n","merging (b' beaut', b'if') -> 998\n","merging (b' beautif', b'ul') -> 999\n"]}],"source":["# train the tokenizer to a vocab size of 1000 and verbose is on so you can see what merges are made by the tokenizer.\n","# I think I set the output cell to optional so maybe you are able to toggle it on and off or maybe not I will have to see when I download this as .ipynb\n","tokenizer.train(tokenizer_text, 1000, verbose = True)"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Adding special tokens for the start of a story and the end of a story</h1>"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:16.722581Z","iopub.status.busy":"2024-03-11T00:21:16.722152Z","iopub.status.idle":"2024-03-11T00:21:16.727827Z","shell.execute_reply":"2024-03-11T00:21:16.726761Z","shell.execute_reply.started":"2024-03-11T00:21:16.722544Z"},"trusted":true},"outputs":[],"source":["# add our special start and end tokens to our tokenizer\n","tokenizer.register_special_tokens([\"<|startofstory|>\", \"<|endofstory|>\"])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:16.729475Z","iopub.status.busy":"2024-03-11T00:21:16.729098Z","iopub.status.idle":"2024-03-11T00:21:16.741937Z","shell.execute_reply":"2024-03-11T00:21:16.740967Z","shell.execute_reply.started":"2024-03-11T00:21:16.729442Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(b'<|startofstory|>', b'<|endofstory|>')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# make sure that our special tokens were added to the vocabulary\n","tokenizer.vocab[tokenizer.vocab_size - 2], tokenizer.vocab[tokenizer.vocab_size - 1]"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Helper functions to ease the training process</h1>"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:26.446221Z","iopub.status.busy":"2024-03-11T00:21:26.445197Z","iopub.status.idle":"2024-03-11T00:21:26.457338Z","shell.execute_reply":"2024-03-11T00:21:26.456103Z","shell.execute_reply.started":"2024-03-11T00:21:26.446174Z"},"trusted":true},"outputs":[],"source":["def build_dataset():\n","    \"\"\"\n","    Builds the training and testing dataset based on the training and validation text pulled from the tinystories dataset\n","    \"\"\"\n","    \n","    \n","    train_data = []\n","    test_data = []\n","    i = 0\n","    \n","    # for each story in train_text\n","    for story in train_text:\n","        \n","        # this is just verbose stuff so I now how fast its going (its excrutiatingly slow)\n","        if (i + 1) % 1000 == 0:\n","            print(i + 1)\n","            print(len(train_data))\n","            \n","        # honestly 4000 stories takes like 10 minutes to compile so this was enough for now\n","        if i > 4000:\n","            break\n","            \n","        # every story starts with the special start token\n","        train_data.append(tokenizer.special_tokens[\"<|startofstory|>\"])\n","        # we extend the list of ids acquired from encoding the story text into our tokens via the tokenizer we trained\n","        train_data.extend(tokenizer.encode(story))\n","        # we add the special end token to end every story\n","        train_data.append(tokenizer.special_tokens[\"<|endofstory|>\"])\n","        \n","        i += 1\n","        \n","    i = 0\n","    \n","    # same deal as the above loop\n","    for story in val_text:\n","        \n","        # 1000 stories gives us an 80/20 split which is pretty standard\n","        if i > 1000:\n","            break\n","            \n","        test_data.append(tokenizer.special_tokens[\"<|startofstory|>\"])\n","        test_data.extend(tokenizer.encode(story))\n","        test_data.append(tokenizer.special_tokens[\"<|endofstory|>\"])\n","        i += 1\n","    \n","    return train_data, test_data"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:31.805830Z","iopub.status.busy":"2024-03-11T00:21:31.805161Z","iopub.status.idle":"2024-03-11T00:21:31.813152Z","shell.execute_reply":"2024-03-11T00:21:31.812052Z","shell.execute_reply.started":"2024-03-11T00:21:31.805797Z"},"trusted":true},"outputs":[],"source":["def get_batch(split):\n","    \"\"\"\n","    Gets a batch of batch_size from the given split\n","    \"\"\"\n","    \n","    # get sthe data by indexing into this dictionary using the given split name\n","    x = {\n","        \"train\": train_data,\n","        \"test\": test_data,\n","    }[split]\n","    \n","    # gets batch_size indices to get a block_sized chunk of training examples\n","    ix = torch.randint(0, len(x) - block_size - 1, (batch_size,))\n","    \n","    Xb, Yb = [], []\n","    \n","    # for each index in ix\n","    for idx in ix:\n","        # the training example of size block_size\n","        Xb.append(train_data[idx: idx + block_size])\n","        # the targets for each example will always be shifted one time step forward\n","        Yb.append(train_data[idx + 1: idx + block_size + 1])\n","        \n","    # some technicalities for getting the batches on the device that the model is on\n","    # using np.array because I once got a warning message saying it was faster to convert a bare list to a tensor by converting to an np array first\n","    # maybe this is wrong but I am not sure and it works the same either way\n","    Xb = torch.tensor(np.array(Xb)).to(device) \n","    Yb = torch.tensor(np.array(Yb)).to(device)\n","    \n","    return Xb, Yb"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:33.623287Z","iopub.status.busy":"2024-03-11T00:21:33.622628Z","iopub.status.idle":"2024-03-11T00:21:33.631006Z","shell.execute_reply":"2024-03-11T00:21:33.629845Z","shell.execute_reply.started":"2024-03-11T00:21:33.623253Z"},"trusted":true},"outputs":[],"source":["def train():\n","    \"\"\"\n","    Training loop for a model\n","    \"\"\"\n","    \n","    # make sure in training mode\n","    model.train()\n","    \n","    # initialize the optimizer using the model parameters and the given learning rate\n","    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n","    \n","    # for each training step\n","    for i in range(total_steps):\n","        \n","        # get the training batch\n","        Xb, Yb = get_batch(\"train\")\n","        \n","        # get the logits and the loss\n","        logits, loss = model(Xb, Yb)\n","        \n","        # reset the gradients\n","        optimizer.zero_grad(set_to_none = True)\n","        # calculate new gradients\n","        loss.backward()\n","        # take a step in the direction of the gradient\n","        optimizer.step()\n","        \n","        # every 100 steps\n","        if (i + 1) % 100 == 0:\n","            # print the current step and current loss\n","            print(\"Step[{}/{}], Loss: {:.4f}\".format(i + 1, total_steps, loss.item()))\n","            \n","        # every eval_interval steps\n","        if (i + 1) % eval_interval == 0:\n","            # estimate the model's current loss on each data split (\"train\" and \"test\")\n","            get_loss()\n","        "]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:39.443541Z","iopub.status.busy":"2024-03-11T00:21:39.443127Z","iopub.status.idle":"2024-03-11T00:21:39.450095Z","shell.execute_reply":"2024-03-11T00:21:39.448959Z","shell.execute_reply.started":"2024-03-11T00:21:39.443509Z"},"trusted":true},"outputs":[],"source":["# decorator so PyTorch doesn't waste compute on calculating unused gradients\n","@torch.no_grad()\n","def get_loss():\n","    \"\"\"\n","    Estimates the loss on each split of data\n","    \"\"\"\n","    \n","    # make sure in eval mode\n","    model.eval()\n","    \n","    # for each split\n","    for split in [\"train\", \"test\"]:\n","        # placeholder losses tensor of size eval_iters\n","        losses = torch.zeros(eval_iters)\n","        \n","        # for each iteration in eval_iters\n","        for i in range(eval_iters):\n","            # get a batch from the split\n","            Xb, Yb = get_batch(split)\n","            # get the logit sand loss\n","            logits, loss = model(Xb, Yb)\n","            # add an entry into our losses tensor for the calculated loss\n","            losses[i] = loss.item()\n","            \n","        # print mean of the losses and the split we just estimated the loss for\n","        print(split, losses.mean().item())"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Initiate the dataset and the model, then train the model on the dataset</h1>"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:21:40.359762Z","iopub.status.busy":"2024-03-11T00:21:40.359010Z","iopub.status.idle":"2024-03-11T00:30:59.336752Z","shell.execute_reply":"2024-03-11T00:30:59.335764Z","shell.execute_reply.started":"2024-03-11T00:21:40.359731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1000\n","308779\n","2000\n","556040\n","3000\n","853034\n","4000\n","1106866\n"]}],"source":["# builds the dataset\n","train_data, test_data = build_dataset()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:30:59.339995Z","iopub.status.busy":"2024-03-11T00:30:59.338937Z","iopub.status.idle":"2024-03-11T00:31:00.698467Z","shell.execute_reply":"2024-03-11T00:31:00.697246Z","shell.execute_reply.started":"2024-03-11T00:30:59.339952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["43.495914 million parameters\n"]},{"data":{"text/plain":["device(type='cuda')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# model hyperparameters\n","\n","total_steps = 2500 # training iterations\n","batch_size = 64 # batch sizes\n","learning_rate = 3e-4 # learning rate\n","n_hidden = 768 # number of dimensions used for embeddings and the hidden layers\n","block_size = 256 # the context length\n","dropout = 0.1 # the percentage of neurons set to 0 when dropout is applied to a tensor\n","eval_interval = total_steps // 4 # how often we will estimate the model's loss\n","eval_iters = 25 # how many iterations we will use to estiamte the model's loss\n","n_layers = 6 # number of blocks that the tokens will pass through\n","n_heads = 6 # number of heads that will pay attention to the tokens (head_size here is n_hidden // n_heads = 128)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # the device to run on (preferrably gpu/cuda, probably the same thing)\n","\n","# initialize the model and immediately move to the device\n","model = GPT().to(device)\n","device # check which device we are working on"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T00:34:15.415800Z","iopub.status.busy":"2024-03-11T00:34:15.415392Z","iopub.status.idle":"2024-03-11T01:04:24.448929Z","shell.execute_reply":"2024-03-11T01:04:24.447856Z","shell.execute_reply.started":"2024-03-11T00:34:15.415769Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step[100/2500], Loss: 5.1399\n","Step[200/2500], Loss: 4.0775\n","Step[300/2500], Loss: 3.5853\n","Step[400/2500], Loss: 3.1255\n","Step[500/2500], Loss: 2.7549\n","Step[600/2500], Loss: 2.4643\n","train 2.274078369140625\n","test 2.2731637954711914\n","Step[700/2500], Loss: 2.0863\n","Step[800/2500], Loss: 1.8307\n","Step[900/2500], Loss: 1.6626\n","Step[1000/2500], Loss: 1.3787\n","Step[1100/2500], Loss: 1.2529\n","Step[1200/2500], Loss: 1.0215\n","train 0.9156764149665833\n","test 0.8992904424667358\n","Step[1300/2500], Loss: 0.7875\n","Step[1400/2500], Loss: 0.6209\n","Step[1500/2500], Loss: 0.5452\n","Step[1600/2500], Loss: 0.4297\n","Step[1700/2500], Loss: 0.3501\n","Step[1800/2500], Loss: 0.2811\n","train 0.2585066854953766\n","test 0.25711798667907715\n","Step[1900/2500], Loss: 0.2660\n","Step[2000/2500], Loss: 0.2347\n","Step[2100/2500], Loss: 0.2203\n","Step[2200/2500], Loss: 0.2015\n","Step[2300/2500], Loss: 0.1934\n","Step[2400/2500], Loss: 0.1846\n","Step[2500/2500], Loss: 0.1629\n","train 0.17298263311386108\n","test 0.17800240218639374\n"]}],"source":["# trains the model using the given hyperparameters above\n","train()"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T01:07:20.949101Z","iopub.status.busy":"2024-03-11T01:07:20.948195Z","iopub.status.idle":"2024-03-11T01:07:26.106890Z","shell.execute_reply":"2024-03-11T01:07:26.105825Z","shell.execute_reply.started":"2024-03-11T01:07:20.949056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Story 1:\n","Once there was a little girl named Liz. She had a teddy bear who lived in her room. \n","\n","One day, Liz was feeling very sad. She wanted to make her dad feel better, but she was bored. She asked her mom, \"Can I have a surprise for you?\" \n","\n","Her mom smiled and said, \"Yes, of course I can! Let's explore some competition to use safety.\"\n","\n","Lizzy gathered up a bubbloves and carefully brought them inside. She showed them how to make cut them into different pieces. Finally after a lot of hard work, she found them. \n","\n","The boy found a big choiceber and things was very happy. He delicious treats had made his friend happy. \n","\n","He sang, They hugged each other and celebrated him. From then on, everyone ate the special crane and the silly poppy.\n","------------------------------------------------------------------------------------------------------------------\n","Story 2:\n","One day, two children were walking in the woods. The children called out, Look! There is so much ice over there! But the other child said, I want to hear it. So the other children all ran to help. They was very worried and slow. \n","\n","The ice-cream man grabbed the children and began to play in their cool. Everyone was having a great time and the children kept getting better and better. The momous children were very happy that their good friend was there to help them heard filled the children playing and cool as the Finally, their doll was happy that their grandma and sure he will landine. The end.\n","------------------------------------------------------------------------------------------------------------------\n","Story 3:\n","The pig was out in the garden, gazing near a big shop. The pig smiled at Lucy and said, \"I think it's nice hereshing shut the shirt again.\" Lucy understood and started to behave. From that day on, she all of the her owns in the spirits, she was also happy and had a mild look great.\n","------------------------------------------------------------------------------------------------------------------\n","Story 4:\n","Once upon a time, there was a little girl called Susie. She was three years old and very curious. She wanted to know about everything.\n","\n","One day Susie was playing in the park with her mom. Susie was playing in the park and when she saw a big, scary dog. She was amazed andikeized he was not alone. \n","\n","Her mom ran to see her door and said, \"Don't worry, Susie. I can sing deliciouses.\" \n","\n","Her mom smiled and said, \"Yes, you can serve you today. I'll be a great here and makeful friends!\" \n","\n","Susie smiled and nodded. With her excitement, she started to look at the idea. She remembered how friend Mummy helped him try something. She got a long scooter and arranged that hard she was a scoot. But, with a scooter!\n","\n","She giggled and had fun moved around the room. She showed her mom how to spray the scooter again and everyone cheered. Anna was so proud that she took her readivery. She hugged her mom, showing her mom and dad and runed. They went to the kitchen.\n","------------------------------------------------------------------------------------------------------------------\n","Story 5:\n"," His dad named Marty, Katy, went for a walk in the park. They saw a man week in the big tree. Kitty was sad because she wanted to take a home. But her mom told her that they were waiting to make a money. Lily was sad and asked, \"We need to take good care of Kitty, just for another one.\" \n","\n","Her mom smiled and said, \"You can help Kitty! She find a good time and made a new friend for her mom. She is happy to have such a nice home and safely!\"\n","\n","Soon, Kitty and they played together in the man but there was too late. They were too tired to go home all sells and jam canty from the end of the ceiling. Everyone had a great day together!\n","------------------------------------------------------------------------------------------------------------------\n"]}],"source":["# generates 5 random stories from the model to test its quality\n","for i in range(5):\n","    print(f\"Story {i + 1}:\")\n","    print(model.generate())\n","    print(\"------------------------------------------------------------------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{},"source":["<h1>The previous model was half of the size of this one and its output was not too bad but it still made no sense so I will now try this larger model below to see how their outputs differ</h1>\n","<p>Some comments I want to make:</p>\n","<ul>\n","    <li>\n","        I cut the block size in half. The previous block size allowed the model to see an entire story in its context length since most stories were just about under 256 tokens so I thought this may help the model with knowing how to end a story that it started. However, I think the model learned to rely on having the full 256 tokens of context in order to make its predictions so when I gave it only one token of context, it was not able to generalize well enough to produce stories in the way that it was trained. Although this may not actually be true because the model trains on every time step of context so it trains on examples from varying context from 1 token to the full 256 tokens. In any case, below I am experimenting with a smaller block size and more resourceful model of double the size based on parameters.\n","    </li>\n","    <li>\n","        After training the larger model I have found that it isnt really any better (at least for the same number of training steps). I tried to train the larger model for another 1000 training steps but my notebook crashed after 1 hour and 45 minutes of run time so I think I will stop here for now.\n","    </li>\n","</ul>"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T01:11:11.202759Z","iopub.status.busy":"2024-03-11T01:11:11.202027Z","iopub.status.idle":"2024-03-11T01:11:13.337914Z","shell.execute_reply":"2024-03-11T01:11:13.336463Z","shell.execute_reply.started":"2024-03-11T01:11:11.202723Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["85.924842 million parameters\n"]},{"data":{"text/plain":["device(type='cuda')"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["total_steps = 2500\n","batch_size = 64\n","learning_rate = 3e-4\n","n_hidden = 768\n","block_size = 128\n","dropout = 0.1\n","eval_interval = total_steps // 4\n","eval_iters = 25\n","n_layers = 12\n","n_heads = 12\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = GPT().to(device)\n","device"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T01:11:19.509678Z","iopub.status.busy":"2024-03-11T01:11:19.509236Z","iopub.status.idle":"2024-03-11T01:40:56.043907Z","shell.execute_reply":"2024-03-11T01:40:56.042685Z","shell.execute_reply.started":"2024-03-11T01:11:19.509644Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step[100/2500], Loss: 5.5434\n","Step[200/2500], Loss: 4.2697\n","Step[300/2500], Loss: 3.7927\n","Step[400/2500], Loss: 3.3661\n","Step[500/2500], Loss: 3.0356\n","Step[600/2500], Loss: 2.7804\n","train 2.652148485183716\n","test 2.636672258377075\n","Step[700/2500], Loss: 2.4743\n","Step[800/2500], Loss: 2.2839\n","Step[900/2500], Loss: 2.0526\n","Step[1000/2500], Loss: 1.9181\n","Step[1100/2500], Loss: 1.8679\n","Step[1200/2500], Loss: 1.7339\n","train 1.6238445043563843\n","test 1.623026728630066\n","Step[1300/2500], Loss: 1.6036\n","Step[1400/2500], Loss: 1.4009\n","Step[1500/2500], Loss: 1.3160\n","Step[1600/2500], Loss: 1.1951\n","Step[1700/2500], Loss: 1.0070\n","Step[1800/2500], Loss: 0.8998\n","train 0.8285203576087952\n","test 0.8397030830383301\n","Step[1900/2500], Loss: 0.7673\n","Step[2000/2500], Loss: 0.6686\n","Step[2100/2500], Loss: 0.5903\n","Step[2200/2500], Loss: 0.5510\n","Step[2300/2500], Loss: 0.4953\n","Step[2400/2500], Loss: 0.4288\n","Step[2500/2500], Loss: 0.3737\n","train 0.4065004587173462\n","test 0.4059840738773346\n"]}],"source":["train()"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T01:40:56.046969Z","iopub.status.busy":"2024-03-11T01:40:56.046536Z","iopub.status.idle":"2024-03-11T01:41:06.831303Z","shell.execute_reply":"2024-03-11T01:41:06.830163Z","shell.execute_reply.started":"2024-03-11T01:40:56.046930Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Story 1:\n","The square shone in the park. He saw a big tree and wanted to share it. It was very fast and it walked very fast. The sun was warm and the cat could not heat. The dog started to sort the rock and splash the water in the sun.\n","\n","The sun was shining on the other person. He thought it was very happy, so he jumped and splashed. Then he heard his mom calling him and tried told them. The staff got in his friends to hear him and@. The dog did not like the staff, but he got tired and carefully. Thepot felt safe to be upset, and the smelly dog was not sad. He wanted to be free again and they would never play together again.\n","------------------------------------------------------------------------------------------------------------------\n","Story 2:\n","One day, Lily's dad and dad took her to a park. There was a big fountain with a ink. He said, \"Lily, you can wash my pet friend. She, you must become nice, but can we play if you do better outside because you believe your dad's dad's problems help. Stell, how could wear other animals like her, and their favorite teddy bear.\" Max said, \"Yes, I feel better now. I sleep in my teddy bear's arm and put a bandage on it.\"\n","\n","The bear was very happy to have His with Max's help. He cleaned the bag and took it to a safe place. From that day on, Max learned to save your tropery when he did something new.\n","------------------------------------------------------------------------------------------------------------------\n","Story 3:\n","Once upon a time there was a group of brave dancer. Every day she would rush to the music and the other animals. One day she wanted to find something fun to play with. She began to step and looked around.\n","\n","The music stopped danger. She was so careful when she started to step on themselves in the sun shade.\n","\n","When the music stopped, Jazz thanked everyone she scurried onto herselves. The unusual thing was compossitive and it was all clean.\n","\n","Winter ran around slowly and makinght of kites splashing in the air. Jac of the room would make the cake look nice and clean. She was happy to have a hidden room and cake.\n","\n","The next day when she was older, her mother said something was in the kitchen. Her mother was ready and she answered \"tonstand!\" she said.\n","\n","The old lady smiled and said, \"My name is OK. I'm excited to show you how to cake the cake together!\" \n","\n","So Jaqu and the girl kept swimming together and laughed in the water.\n","------------------------------------------------------------------------------------------------------------------\n","Story 4:\n","Once upon a time, there was a girl named Lily. She loved to play outside with her friends. One day, she saw a big chest in her backyard. She started to check the chest, but it was too high. She got weak and started to feel sleepy. \n","\n","Lily went to her mom and said, \"Mom, my toys are cold. I can't open it.\"\n","\n","Her mom said, \"It's okay, Lily. But you should be careful to take something you have done. Finging and play with your toys, you can have back tomorrow.\" Lily felt very bad and said, \"Thank you, Okay, Lily. You're can't share my toys again, but you have to say sorry to each other and learn new things. And you have to apologize and help her clean up. Do you understand mean work?\" \n","\n","Lily smiled and said, \"Yes, Mommy, I understand you liked such a good real car. I made it such a delicious message now.\" Can I help you open the food?\" Sara said, \"Of course, you can. But be careful with the bear. He is also in trouble.\" Hesitated, \"Tom, don't care, Lily. It's okay. It's nice to follow you.\"\n","\n","They hugged and nod. They wished they had not gone to the ocean. They wished they had listened to Mommy. They were not too late. They were not helpful.\n","------------------------------------------------------------------------------------------------------------------\n","Story 5:\n","Katy was only three, but she was being alive. One day, she found a box outside her house. Inside the box was something unusual. Mia saw a scary ldd. She was very angry and she knew it was very exciting. She put the folder over the box and inside, but she was too scared to move. \n","Just then, a brave monkey came to the box. It was a surprise. The monster was so kind, but it jumped out from behind the boxes. Mama was so scared that she scurnted around her house.\n","\n","Suddenly, a big scarf that flew away from Lilly's car. It was very embarrassed, but she kept running.\n","\n","From then on, Mama butterfly flew away from in the sky. She felt happy and free from the wind. She smiled and said I wave to the sun againstead you walking back to the wild cat. \n","\n","Mama nodded and held onto the park to make sure thanked him for saving him. He was happy to pretend the day he finally learned an important lesson - to always listen to your parents, so you don't get too close to his energy.\n","------------------------------------------------------------------------------------------------------------------\n"]}],"source":["for i in range(5):\n","    print(f\"Story {i + 1}:\")\n","    print(model.generate())\n","    print(\"------------------------------------------------------------------------------------------------------------------\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4053823,"sourceId":7045013,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
